{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-tune Llama 3.1 (8B) for Turkish Drug Analysis\n",
                "\n",
                "**NOTE**: This notebook MUST be run on a GPU. Ensure you have selected **Runtime > Change runtime type > T4 GPU** if using Colab."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "0baf4fc7",
            "metadata": {},
            "outputs": [
                {
                    "ename": "OSError",
                    "evalue": "[WinError 126] Belirtilen modÃ¼l bulunamadÄ±. Error loading \"c:\\Users\\Emre\\Desktop\\hackathon_chatbot-main\\.venv\\Lib\\site-packages\\torch\\lib\\caffe2_nvrtc.dll\" or one of its dependencies.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall unsloth \u001b[39m\u001b[33m\"\u001b[39m\u001b[33munsloth[colab-new]\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m @ git+https://github.com/unslothai/unsloth.git\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     21\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall --no-deps \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mxformers<0.0.27\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrl<0.9.0\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m peft accelerate bitsandbytes\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Emre\\Desktop\\hackathon_chatbot-main\\.venv\\Lib\\site-packages\\torch\\__init__.py:270\u001b[39m\n\u001b[32m    266\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    268\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# Libraries can either be in path/nvidia/lib_folder/lib or path/lib_folder/lib\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Emre\\Desktop\\hackathon_chatbot-main\\.venv\\Lib\\site-packages\\torch\\__init__.py:266\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    262\u001b[39m             err = ctypes.WinError(ctypes.get_last_error())\n\u001b[32m    263\u001b[39m             err.strerror += (\n\u001b[32m    264\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Error loading \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m or one of its dependencies.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    265\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    268\u001b[39m kernel32.SetErrorMode(prev_error_mode)\n",
                        "\u001b[31mOSError\u001b[39m: [WinError 126] Belirtilen modÃ¼l bulunamadÄ±. Error loading \"c:\\Users\\Emre\\Desktop\\hackathon_chatbot-main\\.venv\\Lib\\site-packages\\torch\\lib\\caffe2_nvrtc.dll\" or one of its dependencies."
                    ]
                }
            ],
            "source": [
                "%%capture\n",
                "# 1. INSTALLATION & SETUP\n",
                "import sys\n",
                "import shutil\n",
                "\n",
                "# STRICT GPU CHECK (No imports required)\n",
                "if not shutil.which('nvidia-smi'):\n",
                "    print(\"\\n\\n\" + \"=\"*60)\n",
                "    print(\"ðŸ›‘ STOPPING EXECUTION: NO NVIDIA GPU DETECTED\")\n",
                "    print(\"=\"*60)\n",
                "    print(\"This notebook relies on 'Unsloth' which requires an NVIDIA GPU.\")\n",
                "    print(\"Your current environment does not have 'nvidia-smi' available.\")\n",
                "    print(\"\\nPLEASE RUN THIS ON GOOGLE COLAB (FREE):\")\n",
                "    print(\"1. Go to https://colab.research.google.com\")\n",
                "    print(\"2. Upload this notebook\")\n",
                "    print(\"3. Runtime > Change runtime type > T4 GPU\")\n",
                "    print(\"=\"*60 + \"\\n\\n\")\n",
                "    raise RuntimeError(\"No GPU detected. Please upload to Google Colab.\")\n",
                "\n",
                "print(\"Installing Unsloth and Dependencies...\")\n",
                "%pip install unsloth \"unsloth[colab-new]\" @ git+https://github.com/unslothai/unsloth.git\n",
                "%pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
                "\n",
                "import torch\n",
                "from unsloth import FastLanguageModel\n",
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "from datasets import load_dataset\n",
                "from unsloth.chat_templates import get_chat_template\n",
                "\n",
                "print(\"âœ… Installation Complete. Libraries Loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Configuration\n",
                "max_seq_length = 2048\n",
                "dtype = None\n",
                "load_in_4bit = True\n",
                "\n",
                "# 3. Load Model\n",
                "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
                "new_model_name = \"llama-3.1-8b-turkish-drug-finetuned\"\n",
                "\n",
                "print(f\"Loading {model_name}...\")\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = model_name,\n",
                "    max_seq_length = max_seq_length,\n",
                "    dtype = dtype,\n",
                "    load_in_4bit = load_in_4bit,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Add LoRA Adapters\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r = 16,\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
                "    lora_alpha = 16,\n",
                "    lora_dropout = 0,\n",
                "    bias = \"none\",\n",
                "    use_gradient_checkpointing = \"unsloth\",\n",
                "    random_state = 3407,\n",
                "    use_rslora = False,\n",
                "    loftq_config = None,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Load Dataset (Upload finetune_dataset.jsonl to Colab files first!)\n",
                "dataset = load_dataset(\"json\", data_files=\"finetune_dataset.jsonl\", split=\"train\")\n",
                "\n",
                "# 6. Format Prompt (Chat Template)\n",
                "tokenizer = get_chat_template(\n",
                "    tokenizer,\n",
                "    chat_template = \"llama-3.1\",\n",
                "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
                ")\n",
                "\n",
                "def formatting_prompts_func(examples):\n",
                "    convos = []\n",
                "    texts = []\n",
                "    for instruction, input_text, output in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n",
                "        user_msg = instruction\n",
                "        if input_text:\n",
                "            user_msg += \"\\n\" + input_text\n",
                "            \n",
                "        convo = [\n",
                "            {\"role\": \"system\", \"content\": \"Sen yardÄ±mcÄ± bir ilaÃ§ asistanÄ±sÄ±n. Her zaman TÃ¼rkÃ§e yanÄ±t ver.\"},\n",
                "            {\"role\": \"user\", \"content\": user_msg},\n",
                "            {\"role\": \"assistant\", \"content\": output},\n",
                "        ]\n",
                "        text = tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False)\n",
                "        texts.append(text)\n",
                "    return { \"text\" : texts, }\n",
                "\n",
                "dataset = dataset.map(formatting_prompts_func, batched = True,)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Train\n",
                "trainer = SFTTrainer(\n",
                "    model = model,\n",
                "    tokenizer = tokenizer,\n",
                "    train_dataset = dataset,\n",
                "    dataset_text_field = \"text\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dataset_num_proc = 2,\n",
                "    packing = False,\n",
                "    args = TrainingArguments(\n",
                "        per_device_train_batch_size = 2,\n",
                "        gradient_accumulation_steps = 4,\n",
                "        warmup_steps = 5,\n",
                "        max_steps = 60,\n",
                "        learning_rate = 2e-4,\n",
                "        fp16 = not torch.cuda.is_bf16_supported(),\n",
                "        bf16 = torch.cuda.is_bf16_supported(),\n",
                "        logging_steps = 1,\n",
                "        optim = \"adamw_8bit\",\n",
                "        weight_decay = 0.01,\n",
                "        lr_scheduler_type = \"linear\",\n",
                "        seed = 3407,\n",
                "        output_dir = \"outputs\",\n",
                "    ),\n",
                ")\n",
                "\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Save & Convert to GGUF\n",
                "model.save_pretrained(new_model_name)\n",
                "tokenizer.save_pretrained(new_model_name)\n",
                "\n",
                "try:\n",
                "    model.save_pretrained_gguf(new_model_name, tokenizer, quantization_method = \"q4_k_m\")\n",
                "    print(f\"Model saved and converted to GGUF in {new_model_name}\")\n",
                "except Exception as e:\n",
                "    print(f\"GGUF conversion failed: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
