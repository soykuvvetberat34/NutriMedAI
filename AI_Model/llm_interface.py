
import requests
import json

class LLMInterface:
    def __init__(self, model_name="llama-3.1-8b-turkish-drug-finetuned"):
        self.base_url = "http://localhost:11434/api/generate"
        self.model_name = model_name
        self.validate_model()

    def validate_model(self):
        """Checks if model exists, falls back to others if not."""
        try:
            # List available models
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                models = [m['name'] for m in response.json().get('models', [])]
                # Normalize names (handle :latest)
                models_base = [m.split(':')[0] for m in models]
                
                if self.model_name not in models and self.model_name not in models_base:
                    print(f"âš ï¸  UYARI: '{self.model_name}' modeli bulunamadÄ±! (Mevcut Modeller: {models})")
                    print("âš ï¸  EÄŸitilmiÅŸ modelinizi 'ollama create' ile oluÅŸturduÄŸunuzdan emin olun.")
                    
                    # Fallback only if absolutely necessary, but warn heavily
                    fallbacks = ["llama3.1", "llama3", "qwen2.5", "gemma2"]
                    for fb in fallbacks:
                        if fb in models or fb in models_base:
                            print(f"ğŸ”„ GeÃ§ici olarak '{fb}' modeline geÃ§iliyor (EÄŸitilmiÅŸ model yok).")
                            self.model_name = fb
                            return
        except Exception as e:
            print(f"âš ï¸  Model kontrolÃ¼ yapÄ±lamadÄ±: {e}")

    def check_connection(self):
        """Checks if Ollama is running."""
        try:
            response = requests.get("http://localhost:11434/", timeout=2)
            return response.status_code == 200
        except:
            return False

    def analyze_direct(self, user_query):
        """
        Sends the query directly to the fine-tuned model without RAG context.
        """
        # System prompt matching the training data style
        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Sen yardÄ±mcÄ± bir ilaÃ§ asistanÄ±sÄ±n. Her zaman TÃ¼rkÃ§e yanÄ±t ver.<|eot_id|><|start_header_id|>user<|end_header_id|>

{user_query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""
        try:
            print(f"ğŸ§  LLM Direkt Analizi: {user_query[:50]}...")
            
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": True,
                "options": {
                    "temperature": 0.3, # Low temperature for factual accuracy
                    "stop": ["<|eot_id|>"]
                }
            }
            
            response = requests.post(self.base_url, json=payload, timeout=120, stream=True)
            response.raise_for_status()
            
            full_response = ""
            for line in response.iter_lines():
                if line:
                    try:
                        json_obj = json.loads(line.decode('utf-8'))
                        chunk = json_obj.get("response", "")
                        full_response += chunk
                        if json_obj.get("done", False):
                            break
                    except json.JSONDecodeError:
                        continue
                        
            return full_response if full_response else "Analiz yanÄ±tÄ± alÄ±namadÄ±."
            
        except Exception as e:
            return f"âš ï¸ LLM HatasÄ±: {str(e)}"

    def analyze_with_qa_context(self, user_query, qa_results=None):
        """
        Analyzes user query with Q&A knowledge base context for better responses.
        qa_results: List of {question, answer, score} from DataLoader.search_general_qa()
        """
        # Build context from Q&A results
        qa_context = ""
        if qa_results:
            qa_context = "\n\n--- Ä°LGÄ°LÄ° BÄ°LGÄ° TABANI ---\n"
            for i, qa in enumerate(qa_results[:2], 1):  # Max 2 relevant Q&A pairs
                qa_context += f"\n**Ã–rnek Soru {i}:** {qa['question']}\n"
                qa_context += f"**Uzman CevabÄ±:** {qa['answer'][:500]}...\n" if len(qa['answer']) > 500 else f"**Uzman CevabÄ±:** {qa['answer']}\n"
            qa_context += "\n--- BÄ°LGÄ° TABANI SONU ---\n"
        
        # Enhanced Turkish prompt
        prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Sen NutriMedAI adlÄ± bir saÄŸlÄ±k asistanÄ±sÄ±n. GÃ¶revin ilaÃ§, besin ve saÄŸlÄ±k konularÄ±nda doÄŸru bilgi vermektir.

KURALLAR:
1. YANITLARIN TAMAMI TÃœRKÃ‡E OLMALIDIR.
2. Bilimsel ve gÃ¼venilir bilgiler sun.
3. Emin olmadÄ±ÄŸÄ±n konularda "bir saÄŸlÄ±k uzmanÄ±na danÄ±ÅŸmanÄ±zÄ± Ã¶neririm" de.
4. KÄ±sa, net ve anlaÅŸÄ±lÄ±r cevaplar ver.
{qa_context}<|eot_id|><|start_header_id|>user<|end_header_id|>

{user_query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""
        try:
            print(f"ğŸ§  LLM Analizi (Q&A Destekli): {user_query[:50]}...")
            
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": True,
                "options": {
                    "temperature": 0.4,
                    "stop": ["<|eot_id|>"]
                }
            }
            
            response = requests.post(self.base_url, json=payload, timeout=120, stream=True)
            response.raise_for_status()
            
            full_response = ""
            for line in response.iter_lines():
                if line:
                    try:
                        json_obj = json.loads(line.decode('utf-8'))
                        chunk = json_obj.get("response", "")
                        full_response += chunk
                        if json_obj.get("done", False):
                            break
                    except json.JSONDecodeError:
                        continue
                        
            return full_response if full_response else "Analiz yanÄ±tÄ± alÄ±namadÄ±."
            
        except Exception as e:
            return f"âš ï¸ LLM HatasÄ±: {str(e)}"

    def analyze_interaction(self, drug_name, context_data, detected_interactions=None):
        """
        Legacy method kept for compatibility but redirects to analyze_direct
        if context is empty or acts as a wrapper.
        """
        # If we are strictly no-RAG, we might just ignore context_data
        # But if the user asked for No-RAG, we should just use the prompt directly.
        # For this specific task, let's use the direct method construction.
        
        return self.analyze_direct(f"{drug_name} hakkÄ±nda bilgi ver. { ' AyrÄ±ca ÅŸu etkileÅŸimler var: ' + str(detected_interactions) if detected_interactions else ''}")

    def get_generic_name(self, brand_name):
        """
        Asks the LLM for the generic name (active ingredient) of a brand.
        Returns: String (e.g., "Ramipril") or None.
        """
        prompt = f"""
        Identify the main active ingredient (generic name) for the drug brand "{brand_name}".
        
        RULES:
        1. Return ONLY the generic name in English.
        2. If you are not 100% sure or if the drug is a local brand you don't know, return "Unknown".
        3. Do NOT guess. Hallucinations are dangerous.
        
        Example:
        Input: "Delix"
        Output: Ramipril
        Input: "UnknownBrand123"
        Output: Unknown
        """
        
        print(f"â³ LLM Etken Madde Sorgusu: '{brand_name}' iÃ§in bekleniyor...", end="", flush=True)
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "num_predict": 50,  # Limit output to 50 tokens
                    "temperature": 0.0   # ZERO temperature for max determinism
                }
            }
            # Reduced timeout since we limited tokens
            response = requests.post(self.base_url, json=payload, timeout=45) 
            response.raise_for_status()
            result = response.json().get("response", "").strip()
            print(" âœ…")
            
            # Basic cleanup (remove dots, extra words if LLM is chatty)
            if "Unknown" in result or len(result) > 50:
                print(f"âš ï¸  LLM CevabÄ± belirsiz: {result}")
                return None
            return result
            
        except requests.exceptions.ConnectionError:
            print(" âŒ (Hata: Ollama baÄŸlantÄ±sÄ± saÄŸlanamadÄ±. LÃ¼tfen uygulamanÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun: localhost:11434)")
            return None
        except requests.exceptions.Timeout:
            print(" âŒ (Zaman aÅŸÄ±mÄ±)")
            return None
        except Exception as e:
            print(f" âŒ (Hata: {e})")
            return None

    def _construct_prompt(self, drug_name, data, detected_interactions=None):
        """Constructs the prompt for the LLM."""
        
        # 1. Pre-translate and Sort Interactions
        interactions_list = []
        if data.get("drug_interactions"):
            # Scoring logic: higher is more severe
            def get_severity_score(text):
                text_lower = text.lower()
                if any(x in text_lower for x in ["life-threatening", "severe", "contraindicated", "serious", "avoid"]):
                    return 3 # Severe
                if any(x in text_lower for x in ["monitor", "risk", "increase", "decrease", "moderate"]):
                    return 2 # Moderate
                return 1 # Minor
            
            # Sort by severity score (descending)
            sorted_interactions = sorted(
                data['drug_interactions'], 
                key=lambda x: get_severity_score(x['effect']), 
                reverse=True
            )
            
            # Take top 10
            top_interactions = sorted_interactions[:10]
            
            for i in top_interactions:
                score = get_severity_score(i['effect'])
                label = "CÄ°DDÄ°" if score == 3 else "ORTA" if score == 2 else "HAFÄ°F"
                interactions_list.append(f"{i['drug']} ({label} EtkileÅŸim)")
                
        interactions_text = ", ".join(interactions_list) if interactions_list else "BelirtilmemiÅŸ"
            
        # 2. Pre-translate Food Interactions (Manual Safety Layer)
        food_data = data.get("food_interactions", [])
        food_list = []
        for item in food_data:
            item = item.replace("Avoid alcohol", "Alkol KULLANMAYINIZ")
            item = item.replace("grapefruit", "Greyfurt KULLANMAYINIZ")
            item = item.replace("Avoid", "Uzak durunuz:")
            item = item.replace("Without food", "AÃ§ karnÄ±na")
            item = item.replace("With food", "Tok karnÄ±na")
            food_list.append(item)
        food_text = "; ".join(food_list) if food_list else "BelirtilmemiÅŸ"
        
        # 3. Clean Description
        desc = data.get("medicine_desc", "N/A")
        if len(desc) > 500:
            desc = desc[:500] + "..."
            
        # 4. Generic Warnings (Contraindications from db_drug_interactions.json)
        gen_warnings = ""
        if "generic_warnings" in data:
            gw = data["generic_warnings"]
            gen_warnings = f"""
            KONTRENDÄ°KASYONLAR: {gw.get('contraindications', 'Yok')}
            GENEL UYARILAR: {gw.get('warnings', 'Yok')}
            """

        # 5. Detected Interactions for Current Session
        current_session_warnings = ""
        if detected_interactions:
            current_session_warnings = "\n        ".join(detected_interactions)
            # Make it very prominent
            current_session_warnings = f"""
            !!! TESPÄ°T EDÄ°LEN KRÄ°TÄ°K Ã‡AKIÅMALAR (ÅU ANKÄ° KULLANIM) !!!
            KullanÄ±cÄ±nÄ±n girdiÄŸi diÄŸer gÄ±dalar/ilaÃ§larla ÅŸu etkileÅŸimler bulundu:
            {current_session_warnings}
            
            Bunu raporunda en baÅŸa, 'ACÄ°L UYARI' baÅŸlÄ±ÄŸÄ±yla yaz!
            """

        # Prepare Professional Clinical Prompt
        prompt = f"""Sen ilaÃ§ etkileÅŸimleri konusunda uzmanlaÅŸmÄ±ÅŸ bir klinik karar destek asistanÄ±sÄ±n.

Analiz sonucunda aÅŸaÄŸÄ±daki ilaÃ§ tespit edilmiÅŸtir:

**Tespit Edilen Ä°laÃ§:** {drug_name}
**Etken Madde:** {data.get('salt_composition', 'BelirtilmemiÅŸ')}

{current_session_warnings}

**VERÄ°TABANI BÄ°LGÄ°LERÄ°:**
- Ä°laÃ§ EtkileÅŸimleri: {interactions_text}
- Besin UyarÄ±larÄ±: {food_text}
- Genel AÃ§Ä±klama: {desc}
- Yan Etkiler: {data.get("side_effects", "BelirtilmemiÅŸ")}
{gen_warnings}

AÅŸaÄŸÄ±daki baÅŸlÄ±klarÄ± iÃ§eren yapÄ±landÄ±rÄ±lmÄ±ÅŸ bir tÄ±bbi rapor oluÅŸtur:

### 1. ğŸš¨ Kritik EtkileÅŸim UyarÄ±sÄ±
Hayati risk taÅŸÄ±yan veya kritik dÃ¼zeyde bir ilaÃ§ etkileÅŸimi olup olmadÄ±ÄŸÄ±nÄ± aÃ§Ä±kÃ§a belirt.
(Varsa tespit edilen kritik etkileÅŸimleri burada vurgula. Yoksa "Kritik dÃ¼zeyde etkileÅŸim tespit edilmedi." yaz.)

### 2. ğŸš« Birlikte KullanÄ±lmamasÄ± Gereken Ä°laÃ§lar
{drug_name} ile orta veya yÃ¼ksek dÃ¼zeyde etkileÅŸime giren ilaÃ§larÄ± listele.
VeritabanÄ±ndaki etkileÅŸim bilgilerini kullanarak spesifik ilaÃ§ isimlerini ve etkileÅŸim dÃ¼zeylerini belirt.

### 3. ğŸ¥¦ GÄ±da ve Yeme-Ä°Ã§me UyarÄ±larÄ±
- Ä°lacÄ±n yemeklerle birlikte veya aÃ§ karnÄ±na alÄ±nÄ±p alÄ±namayacaÄŸÄ±nÄ± aÃ§Ä±kla.
- Greyfurt, alkol veya diÄŸer gÄ±da etkileÅŸimlerini belirt.
- GÄ±danÄ±n emilim (absorbsiyon) Ã¼zerine etkisi olup olmadÄ±ÄŸÄ±nÄ± aÃ§Ä±kla.

### 4. ğŸ’Š Ä°laÃ§ HakkÄ±nda Genel Bilgi
- {drug_name}'in hangi amaÃ§la kullanÄ±ldÄ±ÄŸÄ±nÄ± kÄ±saca aÃ§Ä±kla.
- Etken madde ve ilaÃ§ grubu bilgisi ver.

### 5. âš ï¸ Yan Etkiler ve GÃ¼venlik UyarÄ±larÄ±
- YaygÄ±n yan etkileri listele.
- Ciddi yan etkilerde ne yapÄ±lmasÄ± gerektiÄŸini belirt.

### 6. ğŸ’¡ EczacÄ± Tavsiyesi
Hasta gÃ¼venliÄŸini, ilaÃ§ kombinasyonlarÄ±nÄ± ve klinik deÄŸerlendirmeyi vurgulayan profesyonel bir eczacÄ± Ã¶nerisi sun.

**KURALLAR:**
- Dil aÃ§Ä±k, tÄ±bbi ve profesyonel olsun.
- Ä°ngilizce terimler varsa TÃœRKÃ‡E'ye Ã§evir.
- Gereksiz uyarÄ± ve hukuki aÃ§Ä±klamalardan kaÃ§Ä±n.
- KÄ±sa, net ve hasta gÃ¼venliÄŸini Ã¶n planda tut.
"""
        return prompt

if __name__ == "__main__":
    # Test Stub
    llm = LLMInterface()
    if llm.check_connection():
        print("Ollama is connected.")
        # Dummy context test
        ctx = {
            "drug_interactions": [{"drug": "Aspirin", "effect": "MODERATE"}],
            "food_interactions": ["Avoid Alcohol"],
            "medicine_desc": "Used for anxiety."
        }
        # print(llm.analyze_interaction("Atarax", ctx)) # Uncomment to really test if model is loaded
    else:
        print("Ollama is NOT connected (Expected if not running).")
