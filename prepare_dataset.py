import json
import os
import glob

def format_output(data):
    """
    Formats the drug data into the target Turkish output string,
    replicating the logic from llm_interface.py.
    """
    drug_name = data.get("product_name", "Bilinmiyor")
    
    # 1. Pre-translate Interactions
    interactions_list = []
    if data.get("drug_interactions"):
        try:
            # It might be a string JSON or already a list/dict depending on how it was loaded
            # In data_loader it parses it. Here we are reading raw JSON.
            # Convert string to json if needed
            interactions_raw = data.get("drug_interactions")
            parsed = []
            if isinstance(interactions_raw, str):
                parsed_json = json.loads(interactions_raw)
                # Parse the structure {"drug": [], "brand": [], "effect": []}
                if "drug" in parsed_json and "effect" in parsed_json:
                    for i in range(len(parsed_json["drug"])):
                        parsed.append({
                            "drug": parsed_json["drug"][i],
                            "effect": parsed_json["effect"][i] if i < len(parsed_json["effect"]) else "Unknown"
                        })
            elif isinstance(interactions_raw, list):
                parsed = interactions_raw
            
            for i in parsed:
                effect = i.get('effect', '')
                effect_tr = "CÄ°DDÄ°" if effect == 'SERIOUS' else "ORTA" if effect == 'MODERATE' else "HAFÄ°F"
                interactions_list.append(f"{i.get('drug', '')} ({effect_tr} Risk)")
        except:
            pass
            
    interactions_text = ", ".join(interactions_list) if interactions_list else "BelirtilmemiÅŸ"

    # 2. Pre-translate Food Interactions
    food_data = data.get("food_interactions", [])
    food_list = []
    if isinstance(food_data, list):
        for item in food_data:
            item = item.replace("Avoid alcohol", "Alkol kullanmayÄ±nÄ±z")
            item = item.replace("grapefruit", "greyfurt tÃ¼ketilmemeli")
            item = item.replace("Without food", "AÃ§ karnÄ±na alÄ±nmalÄ±")
            item = item.replace("With food", "Tok karnÄ±na alÄ±nmalÄ±")
            food_list.append(item)
    food_text = "; ".join(food_list) if food_list else "BelirtilmemiÅŸ"

    # 3. Description & Side Effects
    desc = data.get("medicine_desc", "Bilgi bulunamadÄ±.")
    side_effects = data.get("side_effects", "BelirtilmemiÅŸ")

    # Construct the Target Output
    # We want the model to "speak" this directly.
    
    response = f"""# ðŸ’Š Ä°laÃ§ EtkileÅŸimleri
* {interactions_text}

# ðŸ¥¦ GÄ±da ve KullanÄ±m
* {food_text}

# âœ… Ä°laÃ§ HakkÄ±nda (Ã–zet)
* Ne Ä°ÅŸe Yarar: {desc[:200]}...
* Yan Etkiler: {side_effects[:200]}...
"""
    return response

def format_db_interactions(entry):
    """Parses db_drug_interactions.json style entries"""
    name = entry.get("Generic Name", "Bilinmiyor")
    indication = entry.get("Indications", "BelirtilmemiÅŸ")
    side_effects = entry.get("Side Effects", "BelirtilmemiÅŸ")
    warnings = entry.get("Interaction warnings & Precautions", "Yok")
    
    response = f"""# ðŸ’Š Ä°laÃ§ Bilgisi: {name}
* KullanÄ±m AlanÄ±: {indication}
* Yan Etkiler: {side_effects}

# âš ï¸ UyarÄ±lar ve EtkileÅŸimler
* {warnings}
"""
    return response

def format_drug_food(entry):
    """Parses drug-food.json style entries"""
    name = entry.get("name", "Bilinmiyor")
    interactions = entry.get("food_interactions", [])
    
    if not interactions:
        return None
        
    interactions_text = "\n* ".join(interactions)
        
    response = f"""# ðŸ¥¦ {name} ile GÄ±da EtkileÅŸimleri
Bu ilacÄ± kullanÄ±rken dikkat edilmesi gerekenler:
* {interactions_text}
"""
    return response

def process_file(filename, dataset):
    if not os.path.exists(filename):
        print(f"Skipping {filename} (not found)")
        return
        
    print(f"Reading {filename}...")
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ Error reading {filename}: {e}")
        return
        
    count = 0
    for entry in data:
        # Detect Schema
        drug_name = None
        output_text = None
        
        # Schema 1: Main Data (veri*.json, training_data*.json)
        if "product_name" in entry:
            drug_name = entry.get("product_name")
            output_text = format_output(entry)
            
        # Schema 2: DB Interactions (db_drug_interactions.json)
        elif "Generic Name" in entry:
            drug_name = entry.get("Generic Name")
            output_text = format_db_interactions(entry)
            
        # Schema 3: Drug Food (drug-food.json)
        elif "food_interactions" in entry and "name" in entry:
            drug_name = entry.get("name")
            output_text = format_drug_food(entry)
            
        if not drug_name or not output_text: 
            continue
        
        # Create Training Example
        training_example = {
            "instruction": f"Åžu ilaÃ§ hakkÄ±nda bilgi ver: {drug_name}",
            "input": "",
            "output": output_text
        }
        
        dataset.append(training_example)
        count += 1
        
    print(f"Added {count} entries from {filename}")

def main():
    # 1. Merge all data first
    print("ðŸ”„ Merging all data files...")
    # Import here to avoid circular dependency if any, or just replicate simple logic
    # Looking at the file structure, we can just glob here too or assume merge_training_data was run
    # Ideally we should import the merge function or run it.
    
    # Let's just scan everything here to be safe and robust
    files = glob.glob("veri*.json") + glob.glob("training_data*.json") + \
            glob.glob("enriched*.json") + ["db_drug_interactions.json", "drug-food.json"]
            
    # Remove duplicates and filtered files
    files = list(set(files))
    files = [f for f in files if "merged" not in f and "ollama" not in f and "conversations" not in f]
    
    print(f"ðŸŽ¯ Processing {len(files)} files: {files}")
    
    dataset = []
    
    for f in files:
        process_file(f, dataset)
        
    output_file = "finetune_dataset.jsonl"
    print(f"Writing {len(dataset)} examples to {output_file}...")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for entry in dataset:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
            
    print("Done.")

if __name__ == "__main__":
    main()
